# üèõÔ∏è IA Juridique - RAG LegalTech

Syst√®me d'IA juridique sp√©cialis√© en **droit s√©n√©galais** et **droit OHADA**, bas√© sur Retrieval-Augmented Generation (RAG) avec Ollama et ChromaDB.

## üìã Vue d'ensemble

Ce projet impl√©mente une solution RAG pour :
- üìÑ Extraire et indexer des documents juridiques (PDF)
- üîç Rechercher des informations pertinentes par similarit√© s√©mantique
- üß† G√©n√©rer des r√©ponses pr√©cises avec le mod√®le LLaMA 3 (Ollama)
- ‚öñÔ∏è Garantir des r√©ponses bas√©es exclusivement sur les sources fournies

## üöÄ Pr√©requis

- **Python 3.10+**
- **Ollama** install√© ([t√©l√©charger](https://ollama.ai))
- **LLaMA 3** t√©l√©charg√© : `ollama pull llama3`

## üì¶ Installation

### 1. Cloner ou cr√©er l'environnement virtuel

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/macOS
source venv/bin/activate
```

### 2. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

### 3. V√©rifier Ollama

```bash
ollama serve  # D√©marrer le serveur (port 11434)
```

## üìÅ Structure du projet

```
IA_LegalTech/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # Workflow principal
‚îÇ   ‚îú‚îÄ‚îÄ extract_text.py         # Extraction de texte PDF
‚îÇ   ‚îú‚îÄ‚îÄ chunking.py             # Segmentation par articles
‚îÇ   ‚îú‚îÄ‚îÄ metadata.py             # M√©tadonn√©es des documents
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py         # Gestion ChromaDB + embeddings
‚îÇ   ‚îú‚îÄ‚îÄ rag_ollama.py           # Pipeline RAG avec Ollama
‚îÇ   ‚îú‚îÄ‚îÄ enricher.py             # Enrichissement du contexte
‚îÇ   ‚îú‚îÄ‚îÄ run_chunking.py         # Script de chunking
‚îÇ   ‚îú‚îÄ‚îÄ index_chunks.py         # Indexation des chunks
‚îÇ   ‚îú‚îÄ‚îÄ test_rag.py             # Tests du RAG
‚îÇ   ‚îú‚îÄ‚îÄ test_rag_anti-crash.py  # Tests robustes avec timeout
‚îÇ   ‚îî‚îÄ‚îÄ test_search.py          # Tests de recherche vectorielle
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ pdf/                    # Documents juridiques (PDF)
‚îú‚îÄ‚îÄ chroma_db/                  # Base de donn√©es vectorielle persistante
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                   # Ce fichier
```

## üîÑ Workflow

### 1. **Extraction, chunking et Indexation** (run_chunking.py)

```bash
python src/run_chunking.py
```

- Extrait le texte des fichiers PDF
- Nettoie et formate le texte
- Segmente par articles juridiques
- Cr√©e les embeddings avec `sentence-transformers`
- Stocke les chunks dans ChromaDB
- Indexe les documents pour la recherche
- G√©n√®re des m√©tadonn√©es


### 2. **Interrogation du RAG** (test_rag_anti-crash.py)

```bash
python src/test_rag_anti-crash.py
```

- Pose une question juridique
- R√©cup√®re les chunks pertinents via recherche vectorielle
- G√©n√®re une r√©ponse pr√©cise avec qwen2.5:3b
- Inclut un timeout de 60s pour √©viter les blocages

- Sa marche sans probleme executer de fichier alors

```bash
python src/test_rag.py
```

## üõ†Ô∏è Fichiers cl√©s

### `vector_store.py`
Gestion de la base de donn√©es vectorielle :
- ChromaDB pour la persistance
- Embeddings multilingues (paraphrase-multilingual-MiniLM-L12-v2)
- Recherche par similarit√© s√©mantique

```python
from vector_store import LegalVectorStore
vectorstore = LegalVectorStore()
results = vectorstore.query("Question juridique", n_results=2)
```

### `rag_ollama.py`
Pipeline RAG complet :
- R√©cup√©ration du contexte depuis ChromaDB
- Construction du prompt avec contraintes absolues
- Appel √† Ollama avec gestion d'erreurs
- Extraction des r√©f√©rences juridiques

### `extract_text.py`
Extraction de texte PDF :
- Support des documents multilingues
- Nettoyage automatique
- Extraction de m√©tadonn√©es

### `chunking.py`
Segmentation intelligente :
- Division par articles (structure juridique)
- Pr√©servation du contexte
- M√©tadonn√©es par chunk

## ‚öôÔ∏è Configuration

### Param√®tres du RAG (test_rag_anti-crash.py)

```python
K = 5                          # Nombre de chunks r√©cup√©r√©s
MAX_CONTEXT_CHARS = 2500        # Limite de contexte
MAX_RESPONSE_TOKENS = 256      # R√©ponses courtes
MODEL = "qwen2.5:3b"               # Mod√®le LLM
TIMEOUT = 180                   # Timeout en secondes
```

### Param√®tres du RAG (test_rag.py)

```python
MODEL = "llama3"               # Mod√®le LLM
```

### Param√®tres ChromaDB

```python
persist_directory = "../chroma_db"  # Dossier de persistance
collection_name = "ia_juridique"    # Nom de la collection
```

## üß™ Tests

### Test RAG robuste
```bash
python src/test_rag_anti-crash.py
```
Inclut gestion du timeout et des erreurs.

### Test de recherche vectorielle
```bash
python src/test_search.py
```
Valide les embeddings et la recherche.

### Test RAG standard
```bash
python src/test_rag.py
```

## üåê Interface Streamlit

### Installation de Streamlit

Streamlit est d√©j√† inclus dans `requirements.txt`. Si vous ne l'avez pas install√© :

```bash
pip install streamlit
```

### Lancement de l'interface

Avec l'environnement virtuel activ√© :

```bash
streamlit run src/app.py
```

Ou si vous √™tes dans le r√©pertoire `src/` :

```bash
streamlit run app.py
```

### Acc√®s √† l'interface

Une fois lanc√©e, l'interface Streamlit est accessible √† l'adresse :

```
http://localhost:8501
```

### Fonctionnalit√©s de l'interface

L'application `app.py` offre une interface web pour :
- üí¨ Poser des questions juridiques en fran√ßais
- üìö Consulter les documents index√©s
- üîç Visualiser les chunks pertinents retrouv√©s
- ‚öñÔ∏è Recevoir des r√©ponses bas√©es sur le droit s√©n√©galais et OHADA
- üìÑ Voir les sources et r√©f√©rences des r√©ponses

### Configuration du port

Si le port 8501 est occup√©, vous pouvez sp√©cifier un autre port :

```bash
streamlit run src/app.py --server.port 8502
```

### Arr√™t de l'application

Pour arr√™ter le serveur Streamlit, appuyez sur `Ctrl+C` dans le terminal.

## üìä Am√©liorations et optimisations

‚úÖ **Param√©trisation rapide** : K=1, tokens r√©duits, timeouts courts  
‚úÖ **Gestion robuste** : Try-catch, timeouts Ollama, cleanup des processus  
‚úÖ **Multilingue** : Embeddings fran√ßais + anglais  
‚úÖ **Persistance** : ChromaDB sauvegarde automatiquement  

## üö® D√©pannage

### Timeout Ollama
```
subprocess.TimeoutExpired: Command 'ollama run llama3' timed out after 60 seconds
```
**Solution** : R√©duire `MAX_CONTEXT_CHARS` ou `MAX_RESPONSE_TOKENS`

### ChromaDB introuvable
```
FileNotFoundError: chroma_db not found
```
**Solution** : Lancer d'abord `python src/run_chunking.py` pour cr√©er la base

### Ollama non disponible
```
ConnectionRefusedError: [Errno 10061] No connection could be made
```
**Solution** : D√©marrer Ollama : `ollama serve`

## üìö D√©pendances principales

- **chromadb** : Base de donn√©es vectorielle
- **sentence-transformers** : Embeddings multilingues
- **ollama** : Interface LLM locale
- **pypdf** : Extraction PDF
- **pydantic** : Validation de donn√©es

## üìù Conventions

- üîé **Recherche** : K=1-2 chunks max (performance)
- üìè **Contexte** : Max 800-1500 caract√®res
- ‚è±Ô∏è **Timeout** : 60-180 secondes selon le contexte
- üìå **M√©tadonn√©es** : Source + article obligatoires

## üîê S√©curit√© et conformit√©

‚úÖ **Pas d'hallucinations** : R√©ponses bas√©es uniquement sur les sources  
‚úÖ **R√©f√©rences juridiques** : Articles + documents toujours cit√©s  
‚úÖ **Local & Priv√©** : Aucune donn√©e envoy√©e √† l'externe  
‚úÖ **S√©n√©galais/OHADA** : Mod√®le sp√©cialis√© en droit r√©gional  

## üìû Support

Pour toute issue ou am√©lioration, consultez la structure du code et les logs d√©taill√©s dans les fichiers de test.

---

**Derni√®re mise √† jour** : Janvier 2026  
**Status** : Production-ready avec gestion d'erreurs robuste ‚úÖ


# Si vous etes trop parresseur pour lire tout le fichier (mdr) voici un resume pour les commandes a faire 

# 1. Telecharger et Intaller Ollama dans votre PC 

### Installer git et python(3.13.7 ou +3.10) sur votre machine si ce n'est pas encore fait 

# 2. Installer ces 2 modeles suivantes dans votre CMD
--> ollama pull llama3 (modele principal mais un peu lourd)
--> ollama pull qwen2.5:3b (modele secondaire plus leger pour tester notre IA)

# 1. Cr√©er son propre environnement
python -m venv venv

# 2. L'activer
source venv/bin/activate  # Sur Mac/Linux
.\venv\Scripts\activate   # Sur Windows

# 3. Cloner le depot git
git clone https://github.com/mn-code-23/IA_Juridique

# 4. Installer TOUT ton projet d'un coup
pip install -r requirements.txt

# 5. Lancer le fichier test_rag.py
python test_rag.py
- Si ca fait planter votre machine (mdr) arreter l'execution et lancer de fichier
python test_rag_anti-crash.py
